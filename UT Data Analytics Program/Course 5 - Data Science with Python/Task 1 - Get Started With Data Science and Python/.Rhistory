install.packages(“readr”)
"readr"
install.packages(“readr”)
install.packages("readr")
library(readr)
library(readr)
detach("package:readr", unload = TRUE)
library(readr)
library(readr)
install.packages(c("clipr", "foreach", "ggplot2", "hms", "iterators", "lava", "Rcpp", "xml2"))
install.packages("odbc")
# Clear objects if necessary
rm(list = ls())
#Stop Cluster. After performing your tasks, make sure to stop your cluster.
stopCluster(cl)
install.packages("devtools")
devtools::install_github("taiyun/corrplot")
rm(list = ls())
install.packages(c("tidyr", "devtools"))
devtools::install_github("garrettgman/DSR")
install.packages(c("actuar", "amap", "argonDash", "argonR", "bayesplot", "bbmle", "BH", "BMA", "bookdown", "broom", "bs4Dash", "callr", "cartogram", "caTools", "cli", "clubSandwich", "ClusterR", "clusterSim", "covr", "crs", "cubature", "curl", "data.table", "DBI", "DBItest", "dendextend", "Deriv", "DescTools", "deSolve", "DHARMa", "digest", "e1071", "emmeans", "Epi", "FactoMineR", "fda.usc", "FME", "foghorn", "forecast", "fracdiff", "frbs", "future", "geepack", "geometry", "ggeffects", "glmertree", "glmnet", "glmnetUtils", "globals", "gmm", "goftest", "GPareto", "heatmaply", "htmlTable", "igraph", "imbalance", "insight", "JuliaCall", "kohonen", "latticeExtra", "lbfgsb3", "lfe", "listenv", "lmerTest", "loo", "maptools", "maxlike", "MCMCpack", "memisc", "mev", "mice", "miceadds", "mime", "miscTools", "missMDA", "mldr", "mlogit", "mlr", "mlrMBO", "msm", "mstate", "multcomp", "multcompView", "multicool", "MuMIn", "network", "OceanView", "odbc", "OpenImageR", "openxlsx", "optmatch", "ordinal", "osmdata", "parameters", "ParamHelpers", "performance", "plot3D", "plotrix", "plyr", "pre", "prettycode", "prettydoc", "pvclust", "quadprog", "quanteda", "quantreg", "R.cache", "R.utils", "RandomFields", "rasterVis", "Rcmdr", "RcppArmadillo", "recipes", "refund", "repr", "reticulate", "revdbayes", "rgdal", "rlang", "rlecuyer", "RMariaDB", "rmarkdown", "RMySQL", "roxygen2", "RPostgres", "rrcov", "rsconnect", "RSpectra", "RSQLite", "rticles", "rust", "rversions", "selectr", "sjPlot", "sna", "spam", "spam64", "SparseM", "spatialreg", "spatstat", "spatstat.utils", "stm", "survival", "systemfit", "testthat", "tidyverse", "tinytex", "tm", "topicmodels", "TTR", "vctrs", "VGAM", "VineCopula", "visNetwork", "webshot"))
install.packages(c("mldr", "OceanView"))
install.packages(c("mldr", "OceanView"))
install.packages("beepr")
library(beepr)
beep(3)
beep("mario")
install.packages(c("bibtex", "car", "deSolve", "DT", "eha", "ISOcodes", "mldr", "MuMIn", "OceanView", "pillar", "psych", "robust", "rootSolve", "rust", "slam"))
# Only run these examples in interactive R sessions
if (interactive()) {
# Profile some code
profvis({
dat <- data.frame(
x = rnorm(5e4),
y = rnorm(5e4)
)
plot(x ~ y, data = dat)
m <- lm(x ~ y, data = dat)
abline(m, col = "red")
})
# Save a profile to an HTML file
p <- profvis({
dat <- data.frame(
x = rnorm(5e4),
y = rnorm(5e4)
)
plot(x ~ y, data = dat)
m <- lm(x ~ y, data = dat)
abline(m, col = "red")
})
htmlwidgets::saveWidget(p, "profile.html")
# Can open in browser from R
browseURL("profile.html")
}
# Only run these examples in interactive R sessions
# Profile some code
profvis({
dat <- data.frame(
x = rnorm(5e4),
y = rnorm(5e4)
)
plot(x ~ y, data = dat)
m <- lm(x ~ y, data = dat)
abline(m, col = "red")
})
# Save a profile to an HTML file
p <- profvis({
dat <- data.frame(
x = rnorm(5e4),
y = rnorm(5e4)
)
plot(x ~ y, data = dat)
m <- lm(x ~ y, data = dat)
abline(m, col = "red")
})
htmlwidgets::saveWidget(p, "profile.html")
# Can open in browser from R
browseURL("profile.html")
# Title: C3T3 Pipeline
# Last update: 01/06/2020
# File/project name: C3T3 Pipeline.R
# RStudio Project name: Task 3 - Evaluate Techniques for WiFi Locationing.Rproj
###############
# Project Notes
###############
# Now it's time to begin a new project for a new client. Our client is
# developing a system to be deployed on large industrial campuses, in
# shopping malls, et cetera to help people to navigate a complex,
# unfamiliar interior space without getting lost. While GPS works fairly
# reliably outdoors, it generally doesn't work indoors, so a different
# technology is necessary. Our client would like us to investigate the
# feasibility of using "wifi fingerprinting" to determine a person's
# location in indoor spaces. Wifi fingerprinting uses the signals from
# multiple wifi hotspots within the building to determine location,
# analogously to how GPS uses satellite signals. We have been provided
# with a large database of wifi fingerprints for a multi-building
# industrial campus with a location (building, floor, and location ID)
# associated with each fingerprint. Your job is to evaluate multiple
# machine learning models to see which produces the best result,
# enabling us to make a recommendation to the client. If your
# recommended model is sufficiently accurate, it will be incorporated
# into a smartphone app for indoor locationing.
#
# This is a deceptively difficult problem to solve. I'm looking forward
# to seeing what you come up with. After completing your analyses,
# please prepare an internal report on the project for IOT Analytics;
# because this initial report will be delivered to an internal audience,
# it can contain more technical detail than the report we will eventually
# present to the client. I have attached some sample data for you to use
# for your analysis.
# Comment multiple lines
# WIN: CMD + SHIFT + C
###############
# Housekeeping
###############
# Clear objects if necessary
rm(list = ls())
# get working directory
wd <- getwd()
# set working directory
setwd(wd)
dir()
# set a value for seed (to be used in the set.seed function)
seed <- 123
################
# Load packages
################
if(!require(caret)){
install.packages("caret")
library(caret)
}
if(!require(corrplot)){
install.packages("corrplot")
library(corrplot)
}
if(!require(readr)){
install.packages("readr")
library(readr)
}
if(!require(dplyr)){
install.packages("dplyr")
library(dplyr)
}
if(!require(tidyr)){
install.packages("tidyr")
library(tidyr)
}
#####################
# Parallel processing
#####################
#--- for WIN ---#
if(!require(doParallel)){
install.packages("doParallel")
library(doParellel)
}
detectCores()  # detect number of cores
cl <- makeCluster(5)  # select number of cores; 2 in this example
registerDoParallel(cl) # register cluster
getDoParWorkers()  # confirm number of cores being used by RStudio
# Stop Cluster. After performing your tasks, make sure to stop your cluster.
#stopCluster(cl)
###############
# Import data
##############
#### --- Load raw datasets --- ############################
# --- Load Train/Existing data (Dataset 1) --- ############
trainingData <- read.csv("trainingData.csv", stringsAsFactors = FALSE)
class(trainingData)  # "data.frame"
str(trainingData)   #19937 obs. of  529 variables
# --- Load Predict/New data (Dataset 2) --- ###############
validationData <- read.csv("validationData.csv", stringsAsFactors = FALSE)
class(validationData)  # "data.frame"
str(validationData)    #1111 obs. of  529 variables
################
# Evaluate data
################
#--- Dataset 1 - trainingData ---##########################################
#520 WAP columns, then FLOOR, BUILDINGID, SPACEID
names(trainingData)  #WAP001 - WAP520, FLOOR, BUILDINGID, SPACEID
summary(trainingData$SPACEID)#complete data set
head(trainingData)   #complete record including spaceid and relativeposition
tail(trainingData)   #no anomoly for head or tail
# plot - ANY INTERESTING PLOTS FOR OOB DATA??
#hist(existingProduct$x5StarReviews)
#plot(existingProduct$ProductNum, existingProduct$Volume)
#qqnorm(existingProduct$Price)
# check for missing values
anyNA(trainingData)  # FALSE
is.na(trainingData)  # All FALSE
#--- Dataset 2 - validationData ---##########################################
names(validationData)  #WAP001 - WAP520, FLOOR, BUILDINGID, SPACEID
summary(validationData) #note WAP int range -100 - 0 for a signal, 100 == no signal
summary(validationData$SPACEID)#NO VALUES FOR SPACEID IN VALIDATION DATA SET
#might want to consider using lat/long
head(validationData)   #complete except sapceid and relativeposition are indeed 0
tail(validationData)   #complete except sapceid and relativeposition are indeed 0
# plot - ANY INTERESTING PLOTS FOR OOB DATA??
#hist(existingProduct$x5StarReviews)
#plot(existingProduct$ProductNum, existingProduct$Volume)
#qqnorm(existingProduct$Price)
# check for missing values
anyNA(validationData)  # FALSE
is.na(validationData)  # All FALSE
#############
# Preprocess
#############
#--- Dataset 1 - trainingData ---#
#don't need to normalize the data since the data is all the same scale
#don't have categoricval predictor nor predicted valuez that need to
#be converted to binary (0,1) with the dummyVars function
#drop unneeded columns
trainingData$TIMESTAMP <- NULL
trainingData$USERID <- NULL
trainingData$PHONEID <- NULL
trainingData$RELATIVEPOSITION <- NULL
trainingData$LATITUDE <- NULL
trainingData$LONGITUDE <- NULL
head(trainingData)
#DO NEED though to engineer a predicted feature (target) which
#is a composite of building/floor/space - might need to use
#the dummyVar function on this now that I'm thinking about it
#Need to ENGINEER a predicted value by concatenating BUILDING, FLOOR, and SPACEID
trainingDataWCombined = trainingData %>% unite("label_location", FLOOR:SPACEID, na.rm = TRUE, remove = FALSE)
summary(trainingDataWCombined)
summary(trainingDataWCombined$label_location)
#drop buildingid, spaceid, and floor since i have a combined attribute now
trainingDataWCombined$BUILDINGID <- NULL
trainingDataWCombined$SPACEID <- NULL
trainingDataWCombined$FLOOR <- NULL
#convert label_location to a factor attribute type - which is now a number corresponding to level
trainingDataWCombined$label_location <- as.factor(trainingDataWCombined$label_location)
str(trainingDataWCombined$label_location)
#convert back to character strings for label location if needed
fact_character <- levels(trainingDataWCombined$label_location)[as.numeric(trainingDataWCombined$label_location)]
fact_character
str(fact_character)
#DO I NEED TO DUMIFY THIS FACTOR ATTRIBUTE?? WILL KNOW WHEN I RUN ALGOS
#trainingDataWCombined$label_location <- as.integer(trainingDataWCombined$label_location)
#dumify the data - convert categorical predictors to (0,1)
#newDataFrame1 <- dummyVars(" ~ .", data = existingProduct)
#readyData_existingProduct <- data.frame(predict(newDataFrame1, newdata = existingProduct))
#View(readyData_existingProduct)
#str(readyData_existingProduct)
# change data types
#DatasetName$ColumnName <- as.typeofdata(DatasetName$ColumnName)
#for corr to work the dataframe have to be of type numeric
#so converting all columns to numeric in a new variable
#df[] <- lapply(df, as.numeric)
trainingDataWCombinedNumeric <- trainingDataWCombined
trainingDataWCombinedNumeric[] <- lapply(trainingDataWCombined, as.numeric)
str(trainingDataWCombinedNumeric)
str(trainingDataWCombinedNumeric$label_location)
# rename a column
#names(DatasetName)<-c("ColumnName","ColumnName","ColumnName")
# handle missing values
#readyData_existingProduct$BestSellersRank <- NULL #delete attribute BestSellersRank due to NA's
#na.omit(DatasetName$ColumnName)
#na.exclude(DatasetName$ColumnName)
#DatasetName$ColumnName[is.na(DatasetName$ColumnName)] <- mean(DatasetName$ColumnName,na.rm = TRUE)
#create a smaller sample of records from the training set
trainingDataWCombinedNumeric_sample <- trainingDataWCombinedNumeric[sample(1:nrow(trainingDataWCombinedNumeric), 2500, replace=FALSE),]
#Review correlation matrix  -  are there correlations between attributes (not predicted/label attribute)
#trainingDataWCombinedNumeric_sample <- trainingDataWCombinedNumeric_sample[complete.cases(trainingDataWCombinedNumeric_sample), ]
#corrData <- cor(trainingDataWCombinedNumeric_sample[sapply(trainingDataWCombinedNumeric_sample, is.numeric)], use='pairwise')
corrData <- cor(trainingDataWCombinedNumeric_sample, use = "pairwise.complete.obs")
#corrplot(corrData)
corrData
write.csv(corrData, file = "corrData.csv")
volCorrData <- corrData[,"label_location", drop=FALSE]
write.csv(volCorrData, file = "volCorrData.csv")
#Looking for predictors highly correlated to the dependent variable volume
#where correlation greater than .95
# label_location
#nothing above .85 - no correlations, nor colinearity
#readyData_existingProductCorFRemoved <- readyData_existingProduct   # make a copy
#readyData_existingProductCorFRemoved$x5StarReviews <- NULL   # remove 5Star
#str(readyData_existingProductCorFRemoved)
#Looking for colinearity amongst predictors where correlation greater than .90
# x2StarReviews::x1StarReviews   0.951912978
# x3StarReviews::x4StarReviews   0.9372141751
#
# Remove feature with lower correlation to volume
#
# Volume::x1StarReviews   0.255023904
# Volume::x2StarReviews   0.487279328
# Remove feature: x1StarReviews
#readyData_existingProductCorFRemoved$x1StarReviews <- NULL   # remove 1Star
#str(readyData_existingProductCorFRemoved)
#
# Volume::x3StarReviews   0.763373189
# Volume::x4StarReviews   0.879006394
# Remove feature:  x3StarReviews
#readyData_existingProductCorFRemoved$x3StarReviews <- NULL   # remove 1Star
#str(readyData_existingProductCorFRemoved)
# remove obvious features (e.g., ID, other)
#WholeYear7v <- WholeYear   # make a copy
#WholeYear7v$X <- NULL   # remove ID
#str(WholeYear7v) # 35136 obs. of  7 variables
#Scale features as needed weight and height, price and volume features
#names(readyData_existingProductCorFRemoved)
#readyData_existingProductCorFRemoved[c(14, 20:24)] <- scale(readyData_existingProductCorFRemoved[c(14, 20:25)])
#summary(readyData_existingProductCorFRemoved)
# save preprocessed dataset
write.csv(trainingDataWCombinedNumeric_sample, file = "trainingDataWCombinedNumeric_sample.csv")
write.csv(trainingDataWCombinedNumeric, file = "trainingDataWCombinedNumeric.csv")
write.csv(trainingDataWCombined, file = "trainingDataWCombined.csv")
#create a smaller sample of records from the training set
trainingDataWCombinedNumeric_sample <- trainingDataWCombinedNumeric[sample(1:nrow(trainingDataWCombinedNumeric), round(nrow(WholeYear)*.1), replace=FALSE),]
#Review correlation matrix  -  are there correlations between attributes (not predicted/label attribute)
#trainingDataWCombinedNumeric_sample <- trainingDataWCombinedNumeric_sample[complete.cases(trainingDataWCombinedNumeric_sample), ]
#corrData <- cor(trainingDataWCombinedNumeric_sample[sapply(trainingDataWCombinedNumeric_sample, is.numeric)], use='pairwise')
corrData <- cor(trainingDataWCombinedNumeric_sample, use = "pairwise.complete.obs")
#corrplot(corrData)
corrData
#create a smaller sample of records from the training set
trainingDataWCombinedNumeric_sample <- trainingDataWCombinedNumeric[sample(1:nrow(trainingDataWCombinedNumeric), round(nrow(WholeYear)*.1), replace=FALSE),]
trainingDataWCombinedNumeric <- trainingDataWCombined
trainingDataWCombined = trainingData %>% unite("label_location", FLOOR:SPACEID, na.rm = TRUE, remove = FALSE)
# Clear objects if necessary
rm(list = ls())
# get working directory
wd <- getwd()
# set working directory
setwd(wd)
dir()
# set a value for seed (to be used in the set.seed function)
seed <- 123
if(!require(caret)){
install.packages("caret")
library(caret)
}
if(!require(corrplot)){
install.packages("corrplot")
library(corrplot)
}
if(!require(readr)){
install.packages("readr")
library(readr)
}
if(!require(dplyr)){
install.packages("dplyr")
library(dplyr)
}
if(!require(tidyr)){
install.packages("tidyr")
library(tidyr)
}
if(!require(doParallel)){
install.packages("doParallel")
library(doParellel)
}
detectCores()  # detect number of cores
cl <- makeCluster(5)  # select number of cores; 2 in this example
registerDoParallel(cl) # register cluster
getDoParWorkers()  # confirm number of cores being used by RStudio
# --- Load Train/Existing data (Dataset 1) --- ############
trainingData <- read.csv("trainingData.csv", stringsAsFactors = FALSE)
rfFit1 <- readRDS("rfFit1Model.rds") #can skip the above steps and just read the RDS
install.packages(c("dplyr", "glue", "mvtnorm", "partykit", "plyr", "rlang", "survival", "vctrs"))
# Clear objects if necessary
rm(list = ls())
# get working directory
wd <- getwd()
# set working directory
setwd("C:\\Users\\kburr\\Desktop\\UT Data Analytics Program\\Course 5 - Data Science with Python\\Task 1 - Get Started With Data Science and Python")
dir()
# set a value for seed (to be used in the set.seed function)
seed <- 123
if(!require(caret)){
install.packages("caret")
library(caret)
}
if(!require(corrplot)){
install.packages("corrplot")
library(corrplot)
}
if(!require(readr)){
install.packages("readr")
library(readr)
}
if(!require(doParallel)){
install.packages("doParallel")
library(doParellel)
}
detectCores()  # detect number of cores
cl <- makeCluster(2)  # select number of cores; 2 in this example
registerDoParallel(cl) # register cluster
getDoParWorkers()  # confirm number of cores being used by RStudio
# --- Load Train/Existing data (Dataset 1) --- ############
existingProduct <- read.csv("default of credit card clients.csv", stringsAsFactors = FALSE)
class(existingProduct)  # "data.frame"
str(existingProduct)
str(existingProduct)  # 80 obs. of  18 variables
names(existingProduct)
summary(existingProduct)
head(existingProduct)
tail(existingProduct)
transform(existingProduct, LIMIT_BAL = as.numeric(LIMIT_BAL))
transform(existingProduct, X1 = as.numeric(X1))
# --- Load Train/Existing data (Dataset 1) --- ############
existingProduct <- read.csv("default of credit card clients.csv", stringsAsFactors = FALSE)
class(existingProduct)  # "data.frame"
str(existingProduct)
existingProduct.ID <- existingProduct.ID(NULL)
existingProduct$ID <- existingProduct$ID(NULL)
existingProduct$ID <- NULL
transform(existingProduct, LIMIT_BAL = as.numeric(LIMIT_BAL))
str(existingProduct)
existingProduct$ID <- NULL
existingProduct$X <- NULL
str(existingProduct)
View(existingProduct)
# --- Load Train/Existing data (Dataset 1) --- ############
existingProduct <- read.csv("default of credit card clients.csv", header = TRUE, stringsAsFactors = FALSE)
class(existingProduct)  # "data.frame"
str(existingProduct)
existingProduct$X <- NULL
# --- Load Train/Existing data (Dataset 1) --- ############
existingProduct <- read.csv("default of credit card clients.csv", header = TRUE, stringsAsFactors = FALSE)
class(existingProduct)  # "data.frame"
str(existingProduct)
existingProduct$X <- NULL
transform(existingProduct, LIMIT_BAL = as.numeric(LIMIT_BAL))
str(existingProduct)  # 80 obs. of  18 variables
names(existingProduct)
summary(existingProduct)
str(existingProduct)
write.txt(summary(existingProduct), file = "summary results.txt")
write.table(summary(existingProduct), file = "summary results.txt", append = FALSE, sep = " ", dec = ".",
row.names = TRUE, col.names = TRUE)
write.table(summary(existingProduct), file = "summary results.txt", append = FALSE, sep = " ", dec = ".",
row.names = FALSE, col.names = TRUE)
write.table(summary(existingProduct), file = "summary results.txt", append = FALSE, sep = " ", dec = ".",
row.names = FALSE, col.names = FALSE)
summary(existingProduct)
sum(existingProduct$default.payment.next.month)
